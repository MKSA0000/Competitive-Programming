{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bita9bd084df6064062b5ef23b735141db5",
   "display_name": "Python 3.7.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self,input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.w = np.random.normal(size=(input_dim,))\n",
    "        self.b = 0\n",
    "\n",
    "    def __call__(self,x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return sigmoid(np.matmul(x,self.w) + self.b)\n",
    "    \n",
    "    def compute_gradients(self,x,t):\n",
    "        # t is answer data\n",
    "        y = self.forward(x)\n",
    "        delta = y - t\n",
    "        dw = np.matmul(x.T,delta)\n",
    "        db = np.matmul(np.ones(x.shape[0]).T,delta)\n",
    "        return dw, db\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0, loss: 5.08687007525513)\n10, loss: 2.2819203426414854)\n20, loss: 1.8794033873521987)\n30, loss: 1.642085700690544)\n40, loss: 1.4555838803156604)\n50, loss: 1.3028248736327497)\n60, loss: 1.1761739960352886)\n70, loss: 1.0701161618466037)\n80, loss: 0.9803723253394764)\n90, loss: 0.903639454793767)\n99, loss: 0.8435961008671229)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(2)\n",
    "def compute_loss(t,y):\n",
    "    #CrossEntropy\n",
    "    return (-t*np.log(y)-(1-t)*np.log(1-y)).sum()\n",
    "\n",
    "def train_step(x,t):\n",
    "    dw,db = model.compute_gradients(x,t)\n",
    "    model.w = model.w - 0.1 * dw\n",
    "    model.b = model.b - 0.1 * db\n",
    "    loss = compute_loss(t,model(x))\n",
    "    return loss\n",
    "\n",
    "epochs = 100\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "t = np.array([0,1,1,1])\n",
    "for ep in range(epochs):\n",
    "    train_loss = train_step(x,t)\n",
    "    if(ep % 10 == 0 or ep == epochs - 1):\n",
    "        print(f'{ep}, loss: {train_loss})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1125899906842623"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from scipy.special import comb\n",
    "N = 50\n",
    "ans = 0\n",
    "for t in range(1,51):\n",
    "    ans += comb(N,t,exact=True)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}